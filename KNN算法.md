# KNN

k近邻方法是一种惰性学习算法，可以用于回归和分类，它的主要思想是投票机制，对于一个测试实例$x_j$, 我们在有标签的训练数据集上找到和最相近的k个数据，用他们的label进行投票，分类问题则进行表决投票，回归问题使用加权平均或者直接平均的方法。



## 整体介绍

KNN就是通过找寻相近的训练数据集，对于预测数据集，通过找打与他相近的K个相近是训练数据集，并通过加权或者直接平均得出预测数据集的标签，正好和物以类聚人以群分的思想一致；

> 输入:训练数据 $$T={(x_1,y_1),(x_2,y_2),...,(x_N,y_n)}$$ 其中$x_i \in R^n$,是实例的特征向量，$y_i \in Y = {c_1,c_2,...,c_K}$,表示类别，
> 输出: 实例x所属的类别
>
> 1. 根据跟定的距离度量的方法，在T中找到和x最邻近的k个点，记作x的邻域，$N_k(x)$
> 2. 在$N_k(x)$中使用多数表决规则，绝对x的类别y: $$y=argmax_{c_j} \sum_{x_i \in N_k(x)}I(y_i=c_j)$$
> 3. 对于回归问题,得到y $$y= \frac{1}{k} \sum_{x_i \in N_k(x)}y_i$$ 其中$i=1,2,...,N; j=1,2,...,K$.

### 核心公式算法

- 如何计算距离

  通过欧式距离

  $(x_i,y_i)$ 与$(x_j,y_j)$  

  对dis进行开房就能得到两点的距离 这是两维的 多维的求和就行了

  $dis = (x_i - x_j)^2 + (y_i - y_j)^2$   



  对于两个向量$(x_i, x_j)$,一般使用$L_p$距离进行计算。 假设特征空间$X$是n维实数向量空间$R^n$, 其中，$x_i,x_j \in X$, $x_i=(x_i^{(1)},x_i^{(2)},...,x_i^{(n)})，x_j=(x_j^{(1)},x_j^{(2)},...,x_j^{(n)})$，

  $x_i,x_j$的$L_p$距离定义为:

  > $$L_p(x_i,x_j) = \left( \sum_{l=1}^{n}|x_i^{(l)}-x_j^{(l)}|^p \right) ^ {\frac{1}{p}}$$ 这里的$p \geq 1$. 当p=2时候，称为欧氏距离(Euclidean distance), 有 $$L_2(x_i,x_j) = \left( \sum_{l=1}^{n}|x_i^{(l)}-x_j^{(l)}|^2 \right) ^ {\frac{1}{2}}$$ 当p=1时候，称为曼哈顿距离(Manhattan distance), 有 $$L_1(x_i,x_j) = \sum_{l=1}^{n}|x_i^{(l)}-x_j^{(l)}| $$ 当$p=\infty$时候，称为极大距离(infty distance), 表示各个坐标的距离最大值， 有 $$L_p(x_i,x_j) = \max_{l}{n}|x_i^{(l)}-x_j^{(l)}|$$
  >
  > $$L_p(x_i,x_j) = (\sum_{l=1}^{n}|x_{i}^{(l)} -x_{j}^{l}|^p)^{\frac{1}{p}}$$  

- K值的选择 

  k值的选择一般在$K \in[2,20]$中进行选择，一般k和数据有很大关系，通过交叉验证得到一个比较好的K值

  K值还可以表达模型的复杂度，k值越小，表示模型更容易过拟合，k值越大表示模型精度越高，但是学习误差又会比较高；

- 怎样理解k值越大误差越小但是学习误差越大

  估计误差表示最后的结果，k值越大表示，估计的精度越高，自然估计误差就越小，但是由于k越大，分的类别越多，每个类别产生的误差总和就会越来越大

- KD树

  由于KNN每次对于预测函数都要从头到尾进行遍历，然后找打相邻的前k个近邻，所以计算开销较大，时间较慢；所以就产生了KD树



  KD树类似于平衡二叉树的思想





  ## 算法十问

  - 简书下KNN算法原理

    KNN算法核心的思想是通过遍历，找到和预测集最邻近的前k个数据集，通过这几个数据集的标签进行平均或者进行加权处理得出预测集的所属分类；

  - 如何理解KNN中k的取值

    在实际应用中，一般K值取的都是比较小的数值，并通过交叉验证进行调优

  - 在KNN中如何进行高效的匹配查找

    > 线性扫描(数据多时，效率低) 构建数据索引——Clipping和Overlapping两种。前者划分的空间没有重叠，如k-d树；后者划分的空间相互交叠，如R树。（对R树了解很少，可以之后再去了解）

     

  - KNN算法有哪些优点和缺点

    KNN算法简单，既可以做分类也可以做回归，算法精度较高，时间复杂度O(n)；对数据没有假设，对离群不敏感

    缺点：

    计算开销大

  - 不平衡的样本可以给KNN算法的预测造成哪些问题，有什么好的解决办法

    数据较多的时候，可以通过权值进行改变，对于近点赋予较大的权重，距离较远的点赋予较小的权重

  - 为了解决KNN算法计算量过大的问题，可以使用分组方式进行计算

    将样本按照距离分解成组，获得质心，然后计算未知样本到质心的距离，选出最近的一组或者几组
